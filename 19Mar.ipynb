{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533830ba",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d00c3",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range, typically between 0 and 1. It involves subtracting the minimum value of the feature and dividing it by the difference between the maximum and minimum values.\n",
    "\n",
    "Example: Let's say we have a dataset with a feature representing house prices ranging from $100,000 to $1,000,000. By applying Min-Max scaling, we can transform these values to a range of 0 to 1, where $100,000 becomes 0 and $1,000,000 becomes 1. If a house is priced at $500,000, it would be scaled to 0.4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9e69e",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6fc85",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization by vector length, scales the individual samples to have unit norm. Each sample is divided by its Euclidean norm, resulting in a vector of length 1.\n",
    "\n",
    "Example: Consider a dataset with two features: height in centimeters and weight in kilograms. By applying the Unit Vector technique, each data point is divided by its Euclidean norm, resulting in a vector of length 1. This normalization technique ensures that each data point is on the surface of a unit sphere, preserving the direction of the original vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de4c8c",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c5af2",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving the most important information. It achieves this by finding the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Example: Suppose we have a dataset with multiple correlated features representing the physical attributes of individuals, such as height, weight, age, and body measurements. By applying PCA, we can identify the principal components that explain the most significant variance in the data. These components are linear combinations of the original features and can be used as a reduced set of features to represent the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aaa195",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48202e9",
   "metadata": {},
   "source": [
    "PCA can be used for Feature Extraction by identifying the principal components that explain the most significant variance in the data. Instead of using the original features, we can use these principal components as the new set of features. This process extracts the most important information from the dataset while reducing its dimensionality.\n",
    "\n",
    "Example: Suppose we have a dataset with several features representing different aspects of customer behavior in an e-commerce store. By applying PCA, we can identify the principal components that capture the most significant variations in the data. These principal components can represent underlying patterns or latent factors in the customer behavior, effectively extracting essential features from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c0150",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca451b",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "Identify the numeric features in the dataset relevant to the recommendation system, such as price, rating, and delivery time.\n",
    "\n",
    "Determine the range you want to scale the features to, such as 0 to 1 or -1 to 1.\n",
    "\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "\n",
    "For each feature, apply the Min-Max scaling formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula rescales each value to the desired range.\n",
    "\n",
    "Repeat the scaling process for all the numeric features in the dataset.\n",
    "\n",
    "By applying Min-Max scaling, you ensure that the features are on a consistent scale, which can improve the performance of machine learning models and make the features comparable to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562bd8e1",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334fbb5",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset for building a model to predict stock prices using PCA, you would follow these steps:\n",
    "\n",
    "Identify the relevant features in the dataset, such as company financial data and market trends.\n",
    "\n",
    "Normalize or standardize the features to ensure they are on a comparable scale.\n",
    "\n",
    "Apply PCA to the normalized or standardized dataset to find the principal components.\n",
    "\n",
    "Determine the number of principal components to retain based on the desired level of dimensionality reduction and explained variance. This decision can be made by analyzing the cumulative explained variance ratio or using techniques such as scree plot analysis.\n",
    "\n",
    "Select the desired number of principal components based on the analysis in the previous step.\n",
    "\n",
    "Transform the original dataset by projecting it onto the selected principal components to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "By using PCA, you can capture the most important information and patterns in the original dataset while reducing its dimensionality. This can help in simplifying the model and improving computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb8b5b",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b32122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999999999999999, -0.5789473684210525, -0.05263157894736836, 0.47368421052631593, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Original dataset\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "#Create an instance of MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#Reshape the data to a 2D array as MinMaxScaler expects a 2D input\n",
    "\n",
    "data_reshaped = [[val] for val in data]\n",
    "\n",
    "#Fit the scaler on the data and perform the scaling\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_reshaped)\n",
    "\n",
    "#Reshape the scaled data back to a 1D array\n",
    "\n",
    "scaled_data = [val[0] for val in scaled_data]\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49fb2c",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c313e8",
   "metadata": {},
   "source": [
    "To perform Feature Extraction using PCA on a dataset containing the following features: [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of dimensionality reduction and explained variance.\n",
    "\n",
    "Normalize or standardize the features to ensure they are on a comparable scale.\n",
    "\n",
    "Apply PCA to the normalized or standardized dataset and calculate the explained variance ratio for each principal component.\n",
    "\n",
    "Analyze the cumulative explained variance ratio, which represents the proportion of the total variance explained by each principal component and its preceding components.\n",
    "\n",
    "Determine the desired level of explained variance to retain in the reduced-dimensional representation. For example, if you want to retain 95% of the variance, select the number of principal components that achieves or exceeds this threshold.\n",
    "\n",
    "Select the corresponding number of principal components based on the cumulative explained variance ratio analysis.\n",
    "\n",
    "The choice of how many principal components to retain depends on the trade-off between dimensionality reduction and the amount of information retained. Retaining a higher number of principal components will preserve more information but may result in a higher-dimensional representation, while retaining fewer components will lead to more significant dimensionality reduction but may result in a loss of information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
